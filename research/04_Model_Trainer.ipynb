{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Sentence_Generation_Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Sentence_Generation_Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: str\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the configuration\n",
    "config = ModelTrainerConfig(\n",
    "     root_dir=Path('artifacts/model_trainer'),\n",
    "     data_path=Path('artifacts/data_transformation/cleaned_concepts.csv'),\n",
    "     model_ckpt='meta-llama/Llama-2-7b-chat-hf',  # Correct model ID\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgeneration.constants import *\n",
    "from textgeneration.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "     \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "              # Make sure this is an object and not a string\n",
    "\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_trainer_config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chopd\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-20 15:04:39,822: INFO: config: PyTorch version 2.4.1+cu118 available.]\n",
      "[2024-10-20 15:04:40,507: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-20 15:04:40,513: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-20 15:04:40,513: INFO: common: created directory at: artifacts]\n",
      "[2024-10-20 15:04:40,515: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.12s/it]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "c:\\Users\\chopd\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: Generate a meaningful paragraph with at least 750 words using the following concepts: catch, dog, frisbee, throw.\n",
      "\n",
      "As the sun began to set, Jack and his dog, Max, were in the backyard, enjoying a game of catch. Jack had been practicing his throwing skills all week, and it showed as he effortlessly hurled the frisbee high and far towards Max. The dog, with his tail wagging excitedly, ran towards the frisbee and expertly caught it in mid-air. Jack grinned with pride as he watched Max bring the frisbee back to him, ready to be thrown again. This game of catch was more than just a fun activity for Jack and Max, it was a bonding experience that brought them closer together. As they played, Jack couldn't help but think about how much he loved his dog and how much joy he brought to his life. He knew that no matter what, he and Max would always have each other's backs, and that their friendship was truly unbreakable. As the sun dipped below the horizon, Jack and Max continued their game of catch, their bond growing stronger with each throw.\n",
      "Model and tokenizer saved to artifacts/model_trainer\\saved_model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Initialize the ConfigurationManager\n",
    "    config_manager = ConfigurationManager()\n",
    "    \n",
    "    # Get the model trainer configuration\n",
    "    model_trainer_config = config_manager.get_model_trainer_config()\n",
    "\n",
    "\n",
    "\n",
    "    # Load the dataset from the configuration\n",
    "    dataset = pd.read_csv(model_trainer_config.data_path)\n",
    "\n",
    "# Check CUDA availability and set device\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Load the model with CPU offloading and 8-bit precision\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",  # Change to local path if necessary\n",
    "    device_map={\"\": device},  # Manually set device allocation\n",
    "    load_in_8bit=True,  # Enable 8-bit loading for reduced memory usage\n",
    "    torch_dtype=torch.float16,# Use mixed precision for memory efficiency\n",
    "    low_cpu_mem_usage=True,  \n",
    "    )\n",
    "\n",
    "# Optionally, compile the model for faster execution (requires PyTorch 2.0+)\n",
    "    if torch.__version__ >= \"2.0\":\n",
    "       model = torch.compile(model)\n",
    "\n",
    "# Example: Generate text for the first data point\n",
    "    concept_set = dataset['cleaned_concept_set'][0]\n",
    "    \n",
    "     # Example concept set  \n",
    "# Remove tags from concepts (keeping only the base word)\n",
    "#cleaned_concepts = [concept.split('_')[0] for concept in concept_set]\n",
    "\n",
    "# Create prompt\n",
    "    prompt = f\"Generate a meaningful paragraph with at least 750 words using the following concepts: {''.join(concept_set)}.\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)  # Move input_ids to the device\n",
    "\n",
    "# Set pad_token to eos_token to avoid errors\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Generate text\n",
    "    #with torch.no_grad():  # Disable gradient calculation to save memory\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=config_manager.params['TrainingArguments']['max_length'],\n",
    "            temperature=config_manager.params['TrainingArguments']['temperature'],\n",
    "            top_p=config_manager.params['TrainingArguments']['top_p'],\n",
    "            top_k=config_manager.params['TrainingArguments']['top_k'],\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "# Decode and print the generated text\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Generated Sentence: {generated_sentence}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any exceptions and print the error message\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_save_path = os.path.join(model_trainer_config.root_dir, \"saved_model\")\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
