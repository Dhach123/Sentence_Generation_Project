{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Sentence_Generation_Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Sentence_Generation_Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: str\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the configuration\n",
    "config = ModelTrainerConfig(\n",
    "     root_dir=Path('artifacts/model_trainer'),\n",
    "     data_path=Path('artifacts/data_transformation/cleaned_concepts.csv'),\n",
    "     model_ckpt='meta-llama/Llama-2-7b-chat-hf',  # Correct model ID\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgeneration.constants import *\n",
    "from textgeneration.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "     \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "              # Make sure this is an object and not a string\n",
    "\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_trainer_config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chopd\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-18 11:11:05,430: INFO: config: PyTorch version 2.4.1+cu118 available.]\n",
      "[2024-10-18 11:11:05,696: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-18 11:11:05,703: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-10-18 11:11:05,704: INFO: common: created directory at: artifacts]\n",
      "[2024-10-18 11:11:05,706: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:54<00:00, 27.21s/it]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "c:\\Users\\chopd\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: Generate a meaningful paragraph with at least 750 words using the following concepts: car, drive, road, water.\n",
      "\n",
      "As I drove along the winding road, I couldn't help but feel a sense of awe at the sheer vastness of the water below. The car, though a trusty companion, seemed small and insignificant in comparison to the endless expanse of the ocean. The road, too, seemed to fade into the distance as I gazed out at the blue horizon. I had always loved driving, but never had I felt such a profound connection to the natural world as I did at that moment. The smooth, paved road seemed to stretch on forever, a testament to the ingenuity of human engineering, yet also a reminder of the fragility of our existence. As I continued to drive, the water stretching out before me grew more turbulent, the waves crashing against the shore with a ferocity that seemed almost primal. I couldn't help but feel a sense of reverence for the power of nature, and a deep appreciation for the humble car that had carried me thus far.\n",
      "Model and tokenizer saved to artifacts/model_trainer\\saved_model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Initialize the ConfigurationManager\n",
    "    config_manager = ConfigurationManager()\n",
    "    \n",
    "    # Get the model trainer configuration\n",
    "    model_trainer_config = config_manager.get_model_trainer_config()\n",
    "\n",
    "\n",
    "\n",
    "    # Load the dataset from the configuration\n",
    "    dataset = pd.read_csv(model_trainer_config.data_path)\n",
    "\n",
    "# Check CUDA availability and set device\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Load the model with CPU offloading and 8-bit precision\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",  # Change to local path if necessary\n",
    "    device_map={\"\": device},  # Manually set device allocation\n",
    "    load_in_8bit=True,  # Enable 8-bit loading for reduced memory usage\n",
    "    torch_dtype=torch.float16,# Use mixed precision for memory efficiency\n",
    "    low_cpu_mem_usage=True,  \n",
    "    )\n",
    "\n",
    "# Optionally, compile the model for faster execution (requires PyTorch 2.0+)\n",
    "    if torch.__version__ >= \"2.0\":\n",
    "       model = torch.compile(model)\n",
    "\n",
    "# Example: Generate text for the first data point\n",
    "    concept_set = dataset['cleaned_concept_set'][3]  # Example concept set  \n",
    "# Remove tags from concepts (keeping only the base word)\n",
    "#cleaned_concepts = [concept.split('_')[0] for concept in concept_set]\n",
    "\n",
    "# Create prompt\n",
    "    prompt = f\"Generate a meaningful paragraph with at least 750 words using the following concepts: {''.join(concept_set)}.\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)  # Move input_ids to the device\n",
    "\n",
    "# Set pad_token to eos_token to avoid errors\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Generate text\n",
    "    #with torch.no_grad():  # Disable gradient calculation to save memory\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=config_manager.params['TrainingArguments']['max_length'],\n",
    "            temperature=config_manager.params['TrainingArguments']['temperature'],\n",
    "            top_p=config_manager.params['TrainingArguments']['top_p'],\n",
    "            top_k=config_manager.params['TrainingArguments']['top_k'],\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "# Decode and print the generated text\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Generated Sentence: {generated_sentence}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any exceptions and print the error message\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_save_path = os.path.join(model_trainer_config.root_dir, \"saved_model\")\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
