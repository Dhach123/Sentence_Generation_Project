{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Sentence_Generation_Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Sentence_Generation_Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: str\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the configuration\n",
    "config = ModelTrainerConfig(\n",
    "     root_dir=Path('artifacts/model_trainer'),\n",
    "     data_path=Path('artifacts/data_transformation/cleaned_concepts.csv'),\n",
    "     model_ckpt='meta-llama/Llama-2-7b-chat-hf',  # Correct model ID\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgeneration.constants import *\n",
    "from textgeneration.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "     \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "              # Make sure this is an object and not a string\n",
    "\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_trainer_config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1639610548.py, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 43\u001b[1;36m\u001b[0m\n\u001b[1;33m    prompt = f\"Generate a meaningful paragraph with at least 750 words using the following concepts: {''.join(concept_set)}.\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Initialize the ConfigurationManager\n",
    "    config_manager = ConfigurationManager()\n",
    "    \n",
    "    # Get the model trainer configuration\n",
    "    model_trainer_config = config_manager.get_model_trainer_config()\n",
    "\n",
    "\n",
    "\n",
    "    # Load the dataset from the configuration\n",
    "    dataset = pd.read_csv(model_trainer_config.data_path)\n",
    "\n",
    "# Check CUDA availability and set device\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Load the model with CPU offloading and 8-bit precision\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",  # Change to local path if necessary\n",
    "    device_map={\"\": device},  # Manually set device allocation\n",
    "    load_in_8bit=True,  # Enable 8-bit loading for reduced memory usage\n",
    "    torch_dtype=torch.float16,# Use mixed precision for memory efficiency\n",
    "    low_cpu_mem_usage=True,  \n",
    "    )\n",
    "\n",
    "# Optionally, compile the model for faster execution (requires PyTorch 2.0+)\n",
    "    if torch.__version__ >= \"2.0\":\n",
    "       model = torch.compile(model)\n",
    "\n",
    "# Example: Generate text for the first data point\n",
    "    concept_set = dataset['cleaned_concept_set'][0] # Example concept set  \n",
    "# Remove tags from concepts (keeping only the base word)\n",
    "#cleaned_concepts = [concept.split('_')[0] for concept in concept_set]\n",
    "\n",
    "# Create prompt\n",
    "    prompt = f\"Generate a meaningful paragraph with at least 750 words using the following concepts: {''.join(concept_set)}.\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)  # Move input_ids to the device\n",
    "\n",
    "# Set pad_token to eos_token to avoid errors\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Generate text\n",
    "    #with torch.no_grad():  # Disable gradient calculation to save memory\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=config_manager.params['TrainingArguments']['max_length'],\n",
    "            temperature=config_manager.params['TrainingArguments']['temperature'],\n",
    "            top_p=config_manager.params['TrainingArguments']['top_p'],\n",
    "            top_k=config_manager.params['TrainingArguments']['top_k'],\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "# Decode and print the generated text\n",
    "    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"Generated Sentence: {generated_sentence}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle any exceptions and print the error message\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_save_path = os.path.join(model_trainer_config.root_dir, \"saved_model\")\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
